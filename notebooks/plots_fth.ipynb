{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  # 导入 datetime 模块，用于处理日期和时间相关的操作\n",
    "import glob  # 导入 glob 模块，用于查找符合特定规则的文件路径\n",
    "import math  # 导入 math 模块，提供数学计算功能（如平方根、对数等）\n",
    "import os  # 导入 os 模块，用于与操作系统交互（如文件路径操作）\n",
    "\n",
    "from pathlib import Path  # 导入 Path 类，用于更方便地处理文件路径\n",
    "from importlib import reload  # 导入 reload 函数，用于重新加载模块（在调试时常用）\n",
    "\n",
    "import pandas as pd  # 导入 pandas 库，并简称为 pd，用于数据处理和分析\n",
    "import plotly.express as px  # 导入 plotly.express 库，并简称为 px，用于快速绘制交互式图表\n",
    "import numpy as np  # 导入 numpy 库，并简称为 np，用于科学计算和数组操作\n",
    "import matplotlib.pyplot as plt  # 导入 matplotlib.pyplot 库，并简称为 plt，用于绘制静态图表\n",
    "import matplotlib.colors as mcolors  # 导入 matplotlib.colors 模块，用于颜色处理\n",
    "import seaborn as sns  # 导入 seaborn 库，并简称为 sns，用于高级数据可视化\n",
    "import scipy.stats as stats  # 导入 scipy.stats 模块，用于统计分析\n",
    "\n",
    "from plotly.subplots import make_subplots  # 导入 make_subplots 函数，用于创建子图布局\n",
    "\n",
    "from perf_model import PerfModel  # 从 perf_model 模块导入 PerfModel 类，用于性能建模\n",
    "from utils import *  # 从 utils 模块导入所有内容，通常是自定义工具函数或类\n",
    "\n",
    "# 设置 pandas 显示选项，将最大显示列数设置为无限制，避免省略列内容\n",
    "pd.set_option('display.max_columns', None)  # 设置 pandas 的显示选项，确保所有列都能显示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ● %load_ext autoreload 用于加载 autoreload 扩展。\n",
    "# ● %autoreload 2 设置为自动重载所有模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"../results\"\n",
    "plots_dir = \"../plots\"\n",
    "perf_model_path = \"../data/perf_model.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_to_plot = \"llama2-70b\"\n",
    "model_to_plot = \"bloom-176b\"\n",
    "#seed = 9\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_model = PerfModel(perf_model_path, init=True)\n",
    "normalize_model = model_to_plot\n",
    "normalize_hardware = \"a100-80gb\"\n",
    "normalize_tp = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num servers in cluster\n",
      "isopower\n",
      "70 0 70\n",
      "62 5 67\n",
      "53 10 63\n",
      "44 15 59\n",
      "35 20 55\n",
      "26 25 51\n",
      "17 30 47\n",
      "8 35 43\n",
      "0 40 40\n",
      "isocost\n",
      "86 0 86\n",
      "75 5 80\n",
      "64 10 74\n",
      "53 15 68\n",
      "43 20 63\n",
      "32 25 57\n",
      "21 30 51\n",
      "10 35 45\n",
      "0 40 40\n",
      "isopower hhcap\n",
      "0 57 57\n",
      "5 49 54\n",
      "10 42 52\n",
      "15 35 50\n",
      "20 28 48\n",
      "25 21 46\n",
      "30 14 44\n",
      "35 7 42\n",
      "40 0 40\n"
     ]
    }
   ],
   "source": [
    "# isopower\n",
    "print(\"Num servers in cluster\")  # 打印标题，表示以下内容是集群中服务器的数量\n",
    "\n",
    "print(\"isopower\")  # 打印标题，表示以下内容是基于等功率（isopower）的计算结果\n",
    "max_h100 = 40  # 设置最大 H100 服务器数量为 40\n",
    "for num_h100 in range(0, max_h100+1, 5):  # 遍历 H100 服务器数量，从 0 到 40，步长为 5\n",
    "    num_a100 = int(44 * (max_h100 - num_h100) // 24.8)  # 根据公式计算对应的 A100 服务器数量，确保总功率相等\n",
    "    print(num_a100, num_h100, num_a100+num_h100)  # 打印 A100 数量、H100 数量以及总服务器数量\n",
    "\n",
    "# isocost\n",
    "print(\"isocost\")  # 打印标题，表示以下内容是基于等成本（isocost）的计算结果\n",
    "max_h100 = 40  # 设置最大 H100 服务器数量为 40\n",
    "for num_h100 in range(0, max_h100+1, 5):  # 遍历 H100 服务器数量，从 0 到 40，步长为 5\n",
    "    num_a100 = int(4.76 * (max_h100 - num_h100) // 2.21)  # 根据公式计算对应的 A100 服务器数量，确保总成本相等\n",
    "    print(num_a100, num_h100, num_a100+num_h100)  # 打印 A100 数量、H100 数量以及总服务器数量\n",
    "\n",
    "print(\"isopower hhcap\")  # 打印标题，表示以下内容是基于等功率（isopower）的 H100 能力分配计算结果\n",
    "max_h100 = 40  # 设置最大 H100 服务器数量为 40\n",
    "for prompt_h100 in range(0, max_h100+1, 5):  # 遍历用于提示处理的 H100 服务器数量，从 0 到 40，步长为 5\n",
    "    token_h100 = int(44 * (max_h100 - prompt_h100) // 30.8)  # 根据公式计算用于生成令牌的 H100 服务器数量，确保总功率相等\n",
    "    print(prompt_h100, token_h100, prompt_h100+token_h100)  # 打印提示处理 H100 数量、生成令牌 H100 数量以及总 H100 数量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "def get_data(configs, traces, seed, quantiles=[0.5, 0.9, 0.99], model=\"\"):\n",
    "    results = []  # 初始化 结果列表，用于存储每个配置的结果\n",
    "    request_dfs = {}  # 初始化 字典，用于存储每个配置的请求数据\n",
    "    for trace in traces:  # 遍历 所有跟踪文件\n",
    "        for config in configs:  # 遍历 所有配置\n",
    "            name = config[\"name\"]  # 获取 配置名称\n",
    "            scheduler = config[\"scheduler\"]  # 获取 调度器名称\n",
    "            start_state = config[\"start_state\"]  # 获取 初始状态\n",
    "            cluster = config[\"cluster\"]  # 获取 集群配置\n",
    "\n",
    "            # pdb.set_trace() # fth\n",
    "            summary_df = get_summary_data(results_dir, scheduler, start_state, cluster, trace, seed, model=model)  # 获取 汇总数据\n",
    "            request_df = get_request_data(results_dir, scheduler, start_state, cluster, trace, seed, model=model)  # 获取 请求数据\n",
    "            if summary_df is None or request_df is None:  # 如果 汇总数据 或请求数据为空，则跳过\n",
    "                continue\n",
    "\n",
    "            perf_model.add_baseline_perf(request_df, normalize_model, normalize_hardware, normalize_tp)  # 添加 基线性能数据\n",
    "            request_df[\"baseline_e2e\"] = request_df[\"baseline_ttft\"] + request_df[\"baseline_tbt\"] * (request_df[\"token_sizes\"] - 1)  # 计算基线 端到端时间\n",
    "            request_df[\"ttft_slowdown\"] = request_df[\"ttft_times\"] / request_df[\"baseline_ttft\"]  # 计算 TTFT 减速比\n",
    "            request_df[\"tbt_slowdown\"] = request_df[\"tbt_times\"] / request_df[\"baseline_tbt\"]  # 计算 TBT 减速比\n",
    "            request_df[\"e2e_slowdown\"] = request_df[\"response_times\"] / request_df[\"baseline_e2e\"]  # 计算端到端减速比\n",
    "\n",
    "            # 检查是否存在 OOM 文件（out of memory）\n",
    "            oom = False  # 初始化 OOM 标志为 False\n",
    "            if os.path.exists(f\"{results_dir}/{seed}/{start_state}/{trace}/{cluster}/{model}/{scheduler}/oom.csv\"):  # 如果存在 OOM 文件\n",
    "                oom = True  # 设置 OOM 标志为 True\n",
    "\n",
    "            result = {}  # 初始化结果字典\n",
    "            for key, value in config.items():  # 将配置信息添加到结果字典中\n",
    "                result[key] = value\n",
    "            result[\"trace\"] = trace  # 添加跟踪文件名\n",
    "            result[\"seed\"] = seed  # 添加随机种子\n",
    "            for quantile in quantiles:  # 遍历分位数\n",
    "                result[f\"ttft_slowdown_p{int(quantile * 100)}\"] = request_df[\"ttft_slowdown\"].quantile(quantile)  # 计算 TTFT 减速比分位数\n",
    "                result[f\"tbt_slowdown_p{int(quantile * 100)}\"] = request_df[\"tbt_slowdown\"].quantile(quantile)  # 计算 TBT 减速比分位数\n",
    "                result[f\"e2e_slowdown_p{int(quantile * 100)}\"] = request_df[\"e2e_slowdown\"].quantile(quantile)  # 计算端到端减速比分位数\n",
    "            for quantile in quantiles:  # 遍历分位数\n",
    "                result[f\"ttft_times_p{int(quantile * 100)}\"] = summary_df[f\"ttft_times_p{int(quantile * 100)}\"][0]  # 获取 TTFT 时间分位数\n",
    "                result[f\"tbt_times_p{int(quantile * 100)}\"] = summary_df[f\"tbt_times_p{int(quantile * 100)}\"][0]  # 获取 TBT 时间分位数\n",
    "                result[f\"e2e_times_p{int(quantile * 100)}\"] = summary_df[f\"response_times_p{int(quantile * 100)}\"][0]  # 获取端到端时间分位数\n",
    "            result[\"oom\"] = oom  # 添加 OOM 标志\n",
    "\n",
    "            # 保存结果以便稍后创建 DataFrame\n",
    "            results.append(result)  # 将结果添加到结果列表中\n",
    "            request_dfs[f\"{name}_{trace}\"] = request_df  # 将请求数据保存到字典中\n",
    "\n",
    "    results_df = pd.DataFrame(results)  # 将结果列表转换为 DataFrame\n",
    "    return results_df, request_dfs  # 返回结果 DataFrame 和请求数据字典\n",
    "\n",
    "def get_slo(y_var, quantile):\n",
    "    if y_var == \"tbt_slowdown\" or y_var == \"e2e_slowdown\":  # 如果变量是 TBT 或端到端减速比\n",
    "        if quantile == 0.5:  # 如果分位数是 0.5\n",
    "            return 1.25  # 返回 SLO 值 1.25\n",
    "        if quantile == 0.9:  # 如果分位数是 0.9\n",
    "            return 1.5  # 返回 SLO 值 1.5\n",
    "        if quantile == 0.99:  # 如果分位数是 0.99\n",
    "            return 5  # 返回 SLO 值 5\n",
    "    elif y_var == \"ttft_slowdown\":  # 如果变量是 TTFT 减速比\n",
    "        if quantile == 0.5:  # 如果分位数是 0.5\n",
    "            return 2  # 返回 SLO 值 2\n",
    "        if quantile == 0.9:  # 如果分位数是 0.9\n",
    "            return 3  # 返回 SLO 值 3\n",
    "        if quantile == 0.99:  # 如果分位数是 0.99\n",
    "            return 6  # 返回 SLO 值 6\n",
    "    else:\n",
    "        raise Exception(f\"Invalid y_var:quantile {y_var}:{quantile}\")  # 如果变量或分位数无效，抛出异常\n",
    "\n",
    "def get_y_limits(y_var, quantile):\n",
    "    if quantile == 0.5 or quantile == 0.9:  # 如果分位数是 0.5 或 0.9\n",
    "        return {\n",
    "            'bottom': 0,  # 设置 y 轴下限为 0\n",
    "            'top': 4  # 设置 y 轴上限为 4\n",
    "        }\n",
    "    elif quantile == 0.99:  # 如果分位数是 0.99\n",
    "        return {\n",
    "            'bottom': 0,  # 设置 y 轴下限为 0\n",
    "            'top': 8  # 设置 y 轴上限为 8\n",
    "        }\n",
    "    raise Exception(f\"Invalid y_var:quantile {y_var}:{quantile}\")  # 如果变量或分位数无效，抛出异常\n",
    "\n",
    "def plot_y_vs_trace_new(results_df,\n",
    "                        traces,\n",
    "                        y_vars=[\"ttft_times\", \"tbt_times\", \"e2e_times\"],\n",
    "                        y_vars_labels=[\"TTFT\", \"TBT\", \"E2E\"],\n",
    "                        quantiles=[0.5, 0.9, 0.99],\n",
    "                        title=None):\n",
    "    fig, axs = plt.subplots(nrows=len(y_vars),  # 创建子图，行数等于 y 变量数量\n",
    "                            ncols=len(quantiles),  # 列数等于分位数数量\n",
    "                            figsize=(len(quantiles) * 2.5, len(y_vars) * 1.5),  # 设置图像大小\n",
    "                            sharex=True,  # 共享 x 轴\n",
    "                            constrained_layout=True)  # 自动调整布局\n",
    "\n",
    "    # 绘制图表\n",
    "    for y_var in y_vars:  # 遍历 y 变量\n",
    "        for quantile in quantiles:  # 遍历分位数\n",
    "            sns.lineplot(data=results_df,  # 使用 seaborn 绘制折线图\n",
    "                         x=\"trace\",  # x 轴为跟踪文件名\n",
    "                         y=f\"{y_var}_p{int(quantile * 100)}\",  # y 轴为对应分位数的变量\n",
    "                         hue=\"name\",  # 按名称区分颜色\n",
    "                         style=\"name\",  # 按名称区分样式\n",
    "                         markers=True,  # 显示标记点\n",
    "                         markersize=7,  # 设置标记点大小\n",
    "                         ax=axs[y_vars.index(y_var)][quantiles.index(quantile)])  # 指定子图位置\n",
    "\n",
    "    for ax in axs.flatten():  # 遍历所有子图\n",
    "        ax.grid()  # 添加网格线\n",
    "        ax.get_legend().set_visible(False)  # 隐藏图例\n",
    "        ax.set_xlabel(\"Request Rate (req/s)\")  # 设置 x 轴标签\n",
    "        xlabels = [trace.split(\"_\")[2] for trace in traces]  # 提取跟踪文件中的请求速率\n",
    "        ax.set_xticks(ticks=range(0, len(traces)), labels=xlabels)  # 设置 x 轴刻度和标签\n",
    "\n",
    "    # 在图像中心创建一个统一的图例\n",
    "    handles, labels = axs[0][0].get_legend_handles_labels()  # 获取图例句柄和标签\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3, title=\"\", bbox_to_anchor=(0.5, 1.01))  # 添加图例\n",
    "\n",
    "    #axs[0][0].set_yscale(\"log\")  # 设置 y 轴为对数刻度（注释掉）\n",
    "    #axs[0][1].set_yscale(\"log\")\n",
    "    #axs[2][0].set_yscale(\"log\")\n",
    "    #axs[2][1].set_yscale(\"log\")\n",
    "\n",
    "    for y_var in y_vars:  # 遍历 y 变量\n",
    "        for quantile in quantiles:  # 遍历分位数\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].set_ylabel(\n",
    "                f\"Normalized\\np{int(quantile*100)} {y_vars_labels[y_vars.index(y_var)]}\")  # 设置 y 轴标签\n",
    "            #axs[y_vars.index(y_var)][quantiles.index(quantile)].set_ylabel(f\"p{int(quantile*100)} {y_var} (s)\")\n",
    "            y_limits = get_y_limits(y_var, quantile)  # 获取 y 轴范围\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].set_ylim(**y_limits)  # 设置 y 轴范围\n",
    "            slo = get_slo(y_var, quantile)  # 获取 SLO 值\n",
    "            # 添加 SLO 线\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].axhline(y=slo, color=\"red\", linestyle=\"--\")  # 绘制水平线表示 SLO\n",
    "\n",
    "    if title:  # 如果提供了标题\n",
    "        fig.suptitle(title)  # 设置图像标题\n",
    "\n",
    "    plt.margins(x=0)  # 设置 x 轴边距为 0\n",
    "    # 设置分辨率为 300dpi\n",
    "    plt.gcf().set_dpi(300)\n",
    "\n",
    "\n",
    "def plot_y_vs_trace_new_swapped(results_df,\n",
    "                                traces,\n",
    "                                y_vars=[\"ttft_times\", \"tbt_times\", \"e2e_times\"],\n",
    "                                y_vars_labels=[\"TTFT\", \"TBT\", \"E2E\"],\n",
    "                                quantiles=[0.5, 0.9, 0.99],\n",
    "                                title=None):\n",
    "    fig, axs = plt.subplots(nrows=len(quantiles),  # 创建子图，行数等于分位数数量\n",
    "                            ncols=len(y_vars),  # 列数等于 y 变量数量\n",
    "                            figsize=(len(y_vars) * 2.5, len(quantiles) * 1.5),  # 设置图像大小\n",
    "                            sharex=True,  # 共享 x 轴\n",
    "                            constrained_layout=True)  # 自动调整布局\n",
    "\n",
    "    axs = np.array(axs).reshape(len(quantiles), len(y_vars))  # 调整子图数组形状\n",
    "\n",
    "    # 绘制图表\n",
    "    for y_var in y_vars:  # 遍历 y 变量\n",
    "        for quantile in quantiles:  # 遍历分位数\n",
    "            sns.lineplot(data=results_df,  # 使用 seaborn 绘制折线图\n",
    "                         x=\"trace\",  # x 轴为跟踪文件名\n",
    "                         y=f\"{y_var}_p{int(quantile * 100)}\",  # y 轴为对应分位数的变量\n",
    "                         hue=\"name\",  # 按名称区分颜色\n",
    "                         style=\"name\",  # 按名称区分样式\n",
    "                         markers=True,  # 显示标记点\n",
    "                         markersize=7,  # 设置标记点大小\n",
    "                         ax=axs[quantiles.index(quantile)][y_vars.index(y_var)])  # 指定子图位置\n",
    "\n",
    "    for ax in axs.flatten():  # 遍历所有子图\n",
    "        ax.grid()  # 添加网格线\n",
    "        ax.get_legend().set_visible(False)  # 隐藏图例\n",
    "        ax.set_xlabel(\"Request Rate (req/s)\")  # 设置 x 轴标签\n",
    "        xlabels = [trace.split(\"_\")[2] for trace in traces]  # 提取跟踪文件中的请求速率\n",
    "        ax.set_xticks(ticks=range(0, len(traces)), labels=xlabels)  # 设置 x 轴刻度和标签\n",
    "\n",
    "    # 在图像中心创建一个统一的图例\n",
    "    handles, labels = axs[0][0].get_legend_handles_labels()  # 获取图例句柄和标签\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3, title=\"\", bbox_to_anchor=(0.5, 1.01))  # 添加图例\n",
    "\n",
    "    #axs[0][0].set_yscale(\"log\")  # 设置 y 轴为对数刻度（注释掉）\n",
    "    #axs[0][1].set_yscale(\"log\")\n",
    "    #axs[2][0].set_yscale(\"log\")\n",
    "    #axs[2][1].set_yscale(\"log\")\n",
    "\n",
    "    for y_var in y_vars:  # 遍历 y 变量\n",
    "        for quantile in quantiles:  # 遍历分位数\n",
    "            axs[quantiles.index(quantile)][y_vars.index(y_var)].set_ylabel(\n",
    "                f\"Normalized\\np{int(quantile*100)} {y_vars_labels[y_vars.index(y_var)]}\")  # 设置 y 轴标签\n",
    "            #axs[y_vars.index(y_var)][quantiles.index(quantile)].set_ylabel(f\"p{int(quantile*100)} {y_var} (s)\")\n",
    "            y_limits = get_y_limits(y_var, quantile)  # 获取 y 轴范围\n",
    "            axs[quantiles.index(quantile)][y_vars.index(y_var)].set_ylim(**y_limits)  # 设置 y 轴范围\n",
    "            slo = get_slo(y_var, quantile)  # 获取 SLO 值\n",
    "            # 添加 SLO 线\n",
    "            axs[quantiles.index(quantile)][y_vars.index(y_var)].axhline(y=slo, color=\"red\", linestyle=\"--\")  # 绘制水平线表示 SLO\n",
    "\n",
    "    fig.subplots_adjust(left=0)  # 调整左侧边距\n",
    "    if title:  # 如果提供了标题\n",
    "        fig.suptitle(title)  # 设置图像标题\n",
    "\n",
    "    # 设置分辨率为 300dpi\n",
    "    plt.gcf().set_dpi(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slos = {\n",
    "    \"ttft_slowdown_p99\": get_slo(\"ttft_slowdown\", 0.99),\n",
    "    \"tbt_slowdown_p99\": get_slo(\"tbt_slowdown\", 0.99),\n",
    "    \"e2e_slowdown_p99\": get_slo(\"e2e_slowdown\", 0.99),\n",
    "    \"ttft_slowdown_p90\": get_slo(\"ttft_slowdown\", 0.9),\n",
    "    \"tbt_slowdown_p90\": get_slo(\"tbt_slowdown\", 0.9),\n",
    "    \"e2e_slowdown_p90\": get_slo(\"e2e_slowdown\", 0.9),\n",
    "    \"ttft_slowdown_p50\": get_slo(\"ttft_slowdown\", 0.5),\n",
    "    \"tbt_slowdown_p50\": get_slo(\"tbt_slowdown\", 0.5),\n",
    "    \"e2e_slowdown_p50\": get_slo(\"e2e_slowdown\", 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_power_optimal_within_slo(results_df, baseline_system, slos):\n",
    "    \"\"\"\n",
    "    绘制 每个系统的成本和功率的分组条形图，\n",
    "    并将其归一化到 baseline_system。\n",
    "    \"\"\"\n",
    "    # 找到满足 SLO 的配置\n",
    "    configs_within_slo = find_within_slo(results_df, slos)  # 调用函数 筛选出符合 SLO 的配置\n",
    "\n",
    "    # 找到最便宜的 配置\n",
    "    cheapest_configs = find_cheapest(configs_within_slo)  # 调用函数 找到在满足 SLO 的配置中最便宜的配置\n",
    "\n",
    "    # 找到功率最低的 配置\n",
    "    least_power_configs = find_least_power(configs_within_slo)  # 调用函数 找到在满足 SLO 的配置中功率最低的配置\n",
    "\n",
    "    # 将成本和功率 归一化到 baseline_system\n",
    "    baseline_cost = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"cost\"].values[0]  # 获取 基准系统的成本\n",
    "    baseline_power = least_power_configs[least_power_configs[\"system\"] == baseline_system][\"power\"].values[0]  # 获取 基准系统的功率\n",
    "    cheapest_configs[\"cost\"] /= baseline_cost  # 将成本归一化到 基准系统\n",
    "    least_power_configs[\"power\"] /= baseline_power  # 将功率归一化到 基准系统\n",
    "\n",
    "    # 重塑数据框以适应 seaborn\n",
    "    cheapest_configs_melted = cheapest_configs.melt(id_vars='system', value_vars='cost', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将最便宜配置的数据框 转换为长格式\n",
    "    least_power_configs_melted = least_power_configs.melt(id_vars='system', value_vars='power', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将最低功率配置的数据框 转换为长格式\n",
    "    combined_df = pd.concat([cheapest_configs_melted, least_power_configs_melted])  # 合并两个数据框\n",
    "\n",
    "    # 重命名 cost 和 power 为 Cheapest 和 Least Power\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"cost\", \"Cost\")  # 将 \"cost\" 替换为 \"Cost\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"power\", \"Power\")  # 将 \"power\" 替换为 \"Power\"\n",
    "\n",
    "    # 绘制图表\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)  # 创建 一个子图，设置 图像大小和布局\n",
    "    # 按顺序绘制条形图（baseline_system 排在第一位）\n",
    "    sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark', ax=ax)  # 使用 seaborn 绘制分组条形图\n",
    "                #order=[baseline_system, \"Baseline-A100\", \"Splitwise-AA\", \"Splitwise-HA\", \"Splitwise-HH\", \"Splitwise-HHcap\"])\n",
    "    #sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark')\n",
    "    ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 1.01), ncol=3, title=\"\")  # 设置图例位置和样式\n",
    "    xticklabels = results_df[\"system\"].unique()  # 获取所有系统的唯一值作为 x 轴标签\n",
    "    # 在名称和服务器数量之间添加换行符（注释掉的功能）\n",
    "    #xticklabels = [f\"{xticklabel.split('(')[0]}\\n({xticklabel.split('(')[1]}\" for xticklabel in xticklabels]\n",
    "    # 旋转并设置 x 轴刻度\n",
    "    ax.set_xticks(ticks=range(0, len(results_df[\"system\"].unique())), labels=xticklabels, rotation=90)  # 设置 x 轴刻度和标签\n",
    "    # 设置 y 轴次要刻度\n",
    "    ax.set_ylabel(\"Normalized Value\")  # 设置 y 轴标签\n",
    "    ax.set_xlabel(\"\")  # 设置 x 轴标签为空\n",
    "    ax.grid(axis='y')  # 添加 y 轴网格线\n",
    "    ax.minorticks_on()  # 启用次要刻度\n",
    "    ax.set_axisbelow(True)  # 将网格线置于图表下方\n",
    "    # 设置次要刻度增量为 0.25\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.25))  # 设置 y 轴次要刻度间隔\n",
    "    ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black', axis='y')  # 设置次要网格线样式\n",
    "    return combined_df  # 返回合并后的数据框\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_isothroughput_cost_optimal_within_slo(results_df, baseline_system, slos):\n",
    "    \"\"\"\n",
    "    绘制分组条形图，展示每个系统的成本和功耗，并将其归一化到baseline_system\n",
    "    \"\"\"\n",
    "    # 找到满足SLO（服务水平目标）的配置\n",
    "    configs_within_slo = find_within_slo(results_df, slos)  # 调用函数筛选出满足SLO的配置\n",
    "\n",
    "    # 找到最便宜的配置\n",
    "    cheapest_configs = find_cheapest(configs_within_slo)  # 在满足SLO的配置中找到成本最低的配置\n",
    "\n",
    "    # 为每个系统选择唯一的配置\n",
    "    cheapest_configs = cheapest_configs.drop_duplicates(subset=[\"system\"])  # 去重，确保每个系统只保留一个配置\n",
    "\n",
    "    # 将成本和功耗归一化到baseline_system\n",
    "    baseline_servers = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"num_servers\"].values[0]  # 获取基准系统的服务器数量\n",
    "    baseline_cost = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"cost\"].values[0]  # 获取基准系统的成本\n",
    "    baseline_power = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"power\"].values[0]  # 获取基准系统的功耗\n",
    "    cheapest_configs[\"num_servers\"] /= baseline_servers  # 将服务器数量归一化\n",
    "    cheapest_configs[\"cost\"] /= baseline_cost  # 将成本归一化\n",
    "    cheapest_configs[\"power\"] /= baseline_power  # 将功耗归一化\n",
    "\n",
    "    # 将数据框重塑为适合seaborn绘图的格式\n",
    "    num_servers_melted = cheapest_configs.melt(id_vars='system', value_vars='num_servers', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将服务器数量列重塑为长格式\n",
    "    cheapest_melted = cheapest_configs.melt(id_vars='system', value_vars='cost', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将成本列重塑为长格式\n",
    "    power_melted = cheapest_configs.melt(id_vars='system', value_vars='power', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将功耗列重塑为长格式\n",
    "    combined_df = pd.concat([num_servers_melted, cheapest_melted, power_melted])  # 合并所有重塑后的数据框\n",
    "\n",
    "    # 重命名变量名以便更直观地表示\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"num_servers\", \"#Servers\")  # 将变量名从\"num_servers\"改为\"#Servers\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"cost\", \"Cost\")  # 将变量名从\"cost\"改为\"Cost\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"power\", \"Power\")  # 将变量名从\"power\"改为\"Power\"\n",
    "\n",
    "    # 绘制图表\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)  # 创建一个子图，设置图像大小和布局\n",
    "    # 按系统顺序绘制分组条形图\n",
    "    sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark', ax=ax)  # 使用seaborn绘制分组条形图\n",
    "                #order=[baseline_system, \"Baseline-A100\", \"Splitwise-AA\", \"Splitwise-HA\", \"Splitwise-HH\", \"Splitwise-HHcap\"])\n",
    "    #sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark')\n",
    "    ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 1.01), ncol=3, title=\"\")  # 设置图例位置和样式\n",
    "    xticklabels = cheapest_configs[\"name\"].unique()  # 获取唯一的系统名称作为x轴标签\n",
    "    # 在系统名称和服务器数量之间添加换行符\n",
    "    xticklabels = [f\"{xticklabel.split('(')[0]}\\n({xticklabel.split('(')[1]}\" for xticklabel in xticklabels]  # 格式化x轴标签\n",
    "    # 添加换行符以分隔系统名称和服务器数量\n",
    "    #xticklabels = [f\"{xticklabel.split('(')[0]}\\n({xticklabel.split('(')[1]}\" for xticklabel in xticklabels]\n",
    "    # 旋转并设置x轴刻度标签\n",
    "    ax.set_xticks(ticks=range(0, len(results_df[\"system\"].unique())), labels=xticklabels, rotation=90)  # 设置x轴刻度和标签\n",
    "    # 设置y轴次要刻度\n",
    "    ax.set_ylabel(\"Normalized Value\")  # 设置y轴标签\n",
    "    ax.set_xlabel(\"\")  # 设置x轴标签为空\n",
    "    ax.grid(axis='y')  # 显示y轴网格线\n",
    "    ax.minorticks_on()  # 启用次要刻度\n",
    "    ax.set_axisbelow(True)  # 确保网格线在图形下方\n",
    "    # 设置次要刻度增量为0.25\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.25))  # 设置y轴次要刻度间隔\n",
    "    ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black', axis='y')  # 设置次要网格线样式\n",
    "    return combined_df  # 返回合并后的数据框\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_isothroughput_power_optimal_within_slo(results_df, baseline_system, slos):\n",
    "    \"\"\"\n",
    "    绘制 每个系统的服务器数量、成本和功率的分组条形图，\n",
    "    并将其 归一化到 baseline_system。\n",
    "    \"\"\"\n",
    "    # 找到满足 SLO 的配置\n",
    "    configs_within_slo = find_within_slo(results_df, slos)  # 调用函数 筛选出符合 SLO 的配置\n",
    "\n",
    "    # 找到功率 最低的配置\n",
    "    cheapest_configs = find_least_power(configs_within_slo)  # 调用函数 找到在满足 SLO 的配置中功率最低的配置\n",
    "    \n",
    "    # 每个系统 只保留唯一的配置\n",
    "    cheapest_configs = cheapest_configs.drop_duplicates(subset=[\"system\"])  # 删除重复的 系统配置，确保每个系统只保留一个配置\n",
    "\n",
    "    # 将服务器数量、成本和功率归一化到基准系统\n",
    "    baseline_servers = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"num_servers\"].values[0]  # 获取基准系统的服务器数量\n",
    "    baseline_cost = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"cost\"].values[0]  # 获取 基准系统的成本\n",
    "    baseline_power = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"power\"].values[0]  # 获取 基准系统的功率\n",
    "    cheapest_configs[\"num_servers\"] /= baseline_servers  # 将服务器 数量归一化到基准系统\n",
    "    cheapest_configs[\"cost\"] /= baseline_cost  # 将成本 归一化到基准系统\n",
    "    cheapest_configs[\"power\"] /= baseline_power  # 将功率 归一化到基准系统\n",
    "\n",
    "    # 重塑数据框以适应 seaborn\n",
    "    num_servers_melted = cheapest_configs.melt(id_vars='system', value_vars='num_servers', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将服务器数量的数据框 转换为长格式\n",
    "    cheapest_melted = cheapest_configs.melt(id_vars='system', value_vars='cost', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将成本的数据框 转换为长格式\n",
    "    power_melted = cheapest_configs.melt(id_vars='system', value_vars='power', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将功率的数据框 转换为长格式\n",
    "    combined_df = pd.concat([num_servers_melted, cheapest_melted, power_melted])  # 合并 三个数据框\n",
    "\n",
    "    # 重命名 变量名称以便于显示\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"num_servers\", \"#Servers\")  # 将 \"num_servers\" 替换为 \"#Servers\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"cost\", \"Cost\")  # 将 \"cost\" 替换为 \"Cost\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"power\", \"Power\")  # 将 \"power\" 替换为 \"Power\"\n",
    "\n",
    "    # 绘制图表\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)  # 创建一个子图，设置图像大小和布局\n",
    "    # 按顺序绘制条形图（baseline_system 排在第一位）\n",
    "    sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark', ax=ax)  # 使用 seaborn 绘制分组条形图\n",
    "                #order=[baseline_system, \"Baseline-A100\", \"Splitwise-AA\", \"Splitwise-HA\", \"Splitwise-HH\", \"Splitwise-HHcap\"])\n",
    "    #sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark')\n",
    "    ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 1.01), ncol=3, title=\"\")  # 设置图例位置和样式\n",
    "    xticklabels = cheapest_configs[\"name\"].unique()  # 获取所有系统的唯一名称作为 x 轴标签\n",
    "    # 在名称和服务器数量之间添加换行符\n",
    "    xticklabels = [f\"{xticklabel.split('(')[0]}\\n({xticklabel.split('(')[1]}\" for xticklabel in xticklabels]  # 格式化 x 轴标签，添加换行符\n",
    "    # 旋转并设置 x 轴刻度\n",
    "    ax.set_xticks(ticks=range(0, len(results_df[\"system\"].unique())), labels=xticklabels, rotation=90)  # 设置 x 轴刻度和标签\n",
    "    # 设置 y 轴次要刻度\n",
    "    ax.set_ylabel(\"Normalized Value\")  # 设置 y 轴标签\n",
    "    ax.set_xlabel(\"\")  # 设置 x 轴标签为空\n",
    "    ax.grid(axis='y')  # 添加 y 轴网格线\n",
    "    ax.minorticks_on()  # 启用次要刻度\n",
    "    ax.set_axisbelow(True)  # 将网格线置于图表下方\n",
    "    # 设置次要刻度增量为 0.25\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.25))  # 设置 y 轴次要刻度间隔\n",
    "    ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black', axis='y')  # 设置次要网格线样式\n",
    "    return combined_df  # 返回合并后的数据框\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_isothroughput_count_optimal_within_slo(results_df, baseline_system, slos):\n",
    "    \"\"\"\n",
    "    绘制每个系统的服务器数量、成本和功率的分组条形图，\n",
    "    并将其归一化到基准系统。\n",
    "    \"\"\"\n",
    "    # 找到满足 SLO 的配置\n",
    "    configs_within_slo = find_within_slo(results_df, slos)  # 调用函数 筛选出符合 SLO 的配置\n",
    "\n",
    "    # 找到服务器数量最少的配置\n",
    "    cheapest_configs = find_least_count(configs_within_slo)  # 调用函数 找到在满足 SLO 的配置中服务器数量最少的配置\n",
    "\n",
    "    # 每个系统只保留唯一的配置\n",
    "    cheapest_configs = cheapest_configs.drop_duplicates(subset=[\"system\"])  # 删除重复的系统配置，确保每个系统只保留一个配置\n",
    "\n",
    "    # 将服务器数量、成本和功率归一化到基准系统\n",
    "    baseline_servers = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"num_servers\"].values[0]  # 获取基准系统的服务器数量\n",
    "    baseline_cost = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"cost\"].values[0]  # 获取基准系统的成本\n",
    "    baseline_power = cheapest_configs[cheapest_configs[\"system\"] == baseline_system][\"power\"].values[0]  # 获取基准系统的功率\n",
    "    cheapest_configs[\"num_servers\"] /= baseline_servers  # 将服务器数量归一化到基准系统\n",
    "    cheapest_configs[\"cost\"] /= baseline_cost  # 将成本归一化到基准系统\n",
    "    cheapest_configs[\"power\"] /= baseline_power  # 将功率归一化到基准系统\n",
    "\n",
    "    # 重塑数据框以适应 seaborn\n",
    "    num_servers_melted = cheapest_configs.melt(id_vars='system', value_vars='num_servers', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将服务器数量的数据框转换为长格式\n",
    "    cheapest_melted = cheapest_configs.melt(id_vars='system', value_vars='cost', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将成本的数据框转换为长格式\n",
    "    power_melted = cheapest_configs.melt(id_vars='system', value_vars='power', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将功率的数据框转换为长格式\n",
    "    combined_df = pd.concat([num_servers_melted, cheapest_melted, power_melted])  # 合并三个数据框\n",
    "\n",
    "    # 重命名变量名称以便于显示\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"num_servers\", \"#Servers\")  # 将 \"num_servers\" 替换为 \"#Servers\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"cost\", \"Cost\")  # 将 \"cost\" 替换为 \"Cost\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"power\", \"Power\")  # 将 \"power\" 替换为 \"Power\"\n",
    "\n",
    "    # 绘制图表\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)  # 创建 一个子图，设置 图像大小和布局\n",
    "    # 按顺序绘制条形图（基准系统排在第一位）\n",
    "    sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark', ax=ax)  # 使用 seaborn 绘制分组条形图\n",
    "                #order=[baseline_system, \"Baseline-A100\", \"Splitwise-AA\", \"Splitwise-HA\", \"Splitwise-HH\", \"Splitwise-HHcap\"])\n",
    "    #sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark')\n",
    "    ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 1.01), ncol=3, title=\"\")  # 设置图例位置和样式\n",
    "    xticklabels = cheapest_configs[\"name\"].unique()  # 获取所有系统的唯一名称作为 x 轴标签\n",
    "    # 在名称和服务器数量之间 添加换行符\n",
    "    xticklabels = [f\"{xticklabel.split('(')[0]}\\n({xticklabel.split('(')[1]}\" for xticklabel in xticklabels]  # 格式化 x 轴标签，添加换行符\n",
    "    # 旋转并设置 x 轴刻度\n",
    "    ax.set_xticks(ticks=range(0, len(results_df[\"system\"].unique())), labels=xticklabels, rotation=90)  # 设置 x 轴刻度和标签\n",
    "    # 设置 y 轴次要刻度\n",
    "    ax.set_ylabel(\"Normalized Value\")  # 设置 y 轴标签\n",
    "    ax.set_xlabel(\"\")  # 设置 x 轴标签为空\n",
    "    ax.grid(axis='y')  # 添加 y 轴网格线\n",
    "    ax.minorticks_on()  # 启用次要刻度\n",
    "    ax.set_axisbelow(True)  # 将网格线置于图表下方\n",
    "    # 设置次要刻度增量为 0.25\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.25))  # 设置 y 轴次要刻度间隔\n",
    "    ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black', axis='y')  # 设置次要网格线样式\n",
    "    return combined_df  # 返回合并后的数据框\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_throughput_optimal_within_slo(results_df, baseline_system, slos):\n",
    "    \"\"\"\n",
    "    绘制 每个系统的成本和吞吐量的分组条形图，\n",
    "    并将其 归一化到基准系统。\n",
    "    \"\"\"\n",
    "    # 找到满足 SLO 的配置\n",
    "    configs_within_slo = find_within_slo(results_df, slos)  # 调用函数筛选出符合 SLO 的配置\n",
    "\n",
    "    # 找到吞吐量最大的配置\n",
    "    max_throughput_configs = find_max_throughput(configs_within_slo)  # 调用函数找到在满足 SLO 的配置中吞吐量最大的配置\n",
    "    #print(max_throughput_configs)  # 打印调试信息（注释掉）\n",
    "\n",
    "    # 将服务器数量、成本和吞吐量归一化到基准系统\n",
    "    baseline_servers = max_throughput_configs[max_throughput_configs[\"system\"] == baseline_system][\"num_servers\"].values[0]  # 获取 基准系统的服务器数量\n",
    "    baseline_cost = max_throughput_configs[max_throughput_configs[\"system\"] == baseline_system][\"cost\"].values[0]  # 获取 基准系统的成本\n",
    "    baseline_throughput = max_throughput_configs[max_throughput_configs[\"system\"] == baseline_system][\"throughput\"].values[0]  # 获取 基准系统的吞吐量\n",
    "    max_throughput_configs[\"num_servers\"] /= baseline_servers  # 将服务器数量归一化到基准系统\n",
    "    max_throughput_configs[\"cost\"] /= baseline_cost  # 将成本归一化到基准系统\n",
    "    max_throughput_configs[\"throughput\"] /= baseline_throughput  # 将吞吐量归一化到基准系统\n",
    "\n",
    "    # 重塑数据框以适应 seaborn\n",
    "    max_throughput_melted = max_throughput_configs.melt(id_vars='system', value_vars='throughput', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将吞吐量的数据框转换为长格式\n",
    "    cost_melted = max_throughput_configs.melt(id_vars='system', value_vars='cost', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将成本的数据框转换为长格式\n",
    "    num_servers_melted = max_throughput_configs.melt(id_vars='system', value_vars='num_servers',\n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将服务器数量的数据框转换为长格式\n",
    "    combined_df = pd.concat([num_servers_melted, max_throughput_melted, cost_melted])  # 合并三个数据框\n",
    "\n",
    "    # 重命名变量名称以便于显示\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"cost\", \"Cost\")  # 将 \"cost\" 替换为 \"Cost\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"throughput\", \"Throughput\")  # 将 \"throughput\" 替换为 \"Throughput\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"num_servers\", \"#Servers\")  # 将 \"num_servers\" 替换为 \"#Servers\"\n",
    "\n",
    "    # 绘制图表\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)  # 创建一个子图，设置图像大小和布局\n",
    "    # 按顺序绘制条形图（基准系统排在第一位）\n",
    "    sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark', ax=ax)  # 使用 seaborn 绘制分组条形图\n",
    "                #order=[baseline_system, \"Baseline-A100\", \"Splitwise-AA\", \"Splitwise-HA\", \"Splitwise-HH\", \"Splitwise-HHcap\"])\n",
    "    #sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark')\n",
    "    ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 1.01), ncol=3, title=\"\")  # 设置图例位置和样式\n",
    "    xticklabels = combined_df[\"name\"].unique()  # 获取所有系统的唯一名称作为 x 轴标签\n",
    "    # 在名称和服务器数量之间添加换行符\n",
    "    xticklabels = [f\"{xticklabel.split('(')[0]}\\n({xticklabel.split('(')[1]}\" for xticklabel in xticklabels]  # 格式化 x 轴标签，添加换行符\n",
    "    # 旋转并设置 x 轴刻度\n",
    "    ax.set_xticks(ticks=range(0, len(results_df[\"system\"].unique())), labels=xticklabels, rotation=90)  # 设置 x 轴刻度和标签\n",
    "    # 设置 y 轴次要刻度\n",
    "    ax.set_ylabel(\"Normalized Value\")  # 设置 y 轴标签\n",
    "    ax.set_xlabel(\"\")  # 设置 x 轴标签为空\n",
    "    ax.grid(axis='y')  # 添加 y 轴网格线\n",
    "    ax.minorticks_on()  # 启用次要刻度\n",
    "    ax.set_axisbelow(True)  # 将网格线置于图表下方\n",
    "    # 设置次要刻度增量为 0.25\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.25))  # 设置 y 轴次要刻度间隔\n",
    "    ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black', axis='y')  # 设置次要网格线样式\n",
    "    return combined_df  # 返回合并后的数据框\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power_throughput_optimal_within_slo(results_df, baseline_system, slos):\n",
    "    \"\"\"\n",
    "    绘制分组条形图，展示每个系统的功耗和吞吐量，并将其归一化到baseline_system\n",
    "    \"\"\"\n",
    "    # 找到满足SLO（服务水平目标）的配置\n",
    "    configs_within_slo = find_within_slo(results_df, slos)  # 调用函数筛选出满足SLO的配置\n",
    "\n",
    "    # 找到吞吐量最大的配置\n",
    "    max_throughput_configs = find_max_throughput(configs_within_slo)  # 在满足SLO的配置中找到吞吐量最大的配置\n",
    "    #print(max_throughput_configs)  # 打印最大吞吐量配置以供调试（可选）\n",
    "\n",
    "    # 将功耗和吞吐量归一化到baseline_system\n",
    "    baseline_servers = max_throughput_configs[max_throughput_configs[\"system\"] == baseline_system][\"num_servers\"].values[0]  # 获取基准系统的服务器数量\n",
    "    baseline_power = max_throughput_configs[max_throughput_configs[\"system\"] == baseline_system][\"power\"].values[0]  # 获取基准系统的功耗\n",
    "    baseline_throughput = max_throughput_configs[max_throughput_configs[\"system\"] == baseline_system][\"throughput\"].values[0]  # 获取基准系统的吞吐量\n",
    "    max_throughput_configs[\"num_servers\"] /= baseline_servers  # 将服务器数量归一化\n",
    "    max_throughput_configs[\"power\"] /= baseline_power  # 将功耗归一化\n",
    "    max_throughput_configs[\"throughput\"] /= baseline_throughput  # 将吞吐量归一化\n",
    "\n",
    "    # 将数据框重塑为适合seaborn绘图的格式\n",
    "    max_throughput_melted = max_throughput_configs.melt(id_vars='system', value_vars='throughput', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将吞吐量列重塑为长格式\n",
    "    power_melted = max_throughput_configs.melt(id_vars='system', value_vars='power', \n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将功耗列重塑为长格式\n",
    "    num_servers_melted = max_throughput_configs.melt(id_vars='system', value_vars='num_servers',\n",
    "                            var_name='variable', \n",
    "                            value_name='value')  # 将服务器数量列重塑为长格式\n",
    "    combined_df = pd.concat([num_servers_melted, max_throughput_melted, power_melted])  # 合并所有重塑后的数据框\n",
    "\n",
    "    # 重命名变量名以便更直观地表示\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"power\", \"Power\")  # 将变量名从\"power\"改为\"Power\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"throughput\", \"Throughput\")  # 将变量名从\"throughput\"改为\"Throughput\"\n",
    "    combined_df[\"variable\"] = combined_df[\"variable\"].replace(\"num_servers\", \"#Servers\")  # 将变量名从\"num_servers\"改为\"#Servers\"\n",
    "\n",
    "    # 绘制图表\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 3), constrained_layout=True)  # 创建一个子图，设置图像大小和布局\n",
    "    # 按系统顺序绘制分组条形图\n",
    "    sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark', ax=ax)  # 使用seaborn绘制分组条形图\n",
    "                #order=[baseline_system, \"Baseline-A100\", \"Splitwise-AA\", \"Splitwise-HA\", \"Splitwise-HH\", \"Splitwise-HHcap\"])\n",
    "    #sns.barplot(data=combined_df, x='system', y='value', hue='variable', palette='dark')\n",
    "    ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, 1.01), ncol=3, title=\"\")  # 设置图例位置和样式\n",
    "    xticklabels = results_df[\"name\"].unique()  # 获取唯一的系统名称作为x轴标签\n",
    "    # 在系统名称和服务器数量之间添加换行符\n",
    "    xticklabels = [f\"{xticklabel.split('(')[0]}\\n({xticklabel.split('(')[1]}\" for xticklabel in xticklabels]  # 格式化x轴标签\n",
    "    # 旋转并设置x轴刻度标签\n",
    "    ax.set_xticks(ticks=range(0, len(results_df[\"system\"].unique())), labels=xticklabels, rotation=90)  # 设置x轴刻度和标签\n",
    "    ax.set_ylabel(\"Normalized Value\")  # 设置y轴标签\n",
    "    ax.set_xlabel(\"\")  # 设置x���标签为空\n",
    "    ax.grid(axis='y')  # 显示y轴网格线\n",
    "    ax.minorticks_on()  # 启用次要刻度\n",
    "    ax.set_axisbelow(True)  # 确保网格线在图形下方\n",
    "    # 设置次要刻度增量为0.25\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.25))  # 设置y轴次要刻度间隔\n",
    "    ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black', axis='y')  # 设置次要网格线样式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Call--\n",
      "> \u001b[0;32m/disk1/futianhao/software2/splitwise-sim/notebooks/utils.py\u001b[0m(158)\u001b[0;36mget_summary_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    156 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    157 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 158 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    159 \u001b[0;31m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    160 \u001b[0;31m        \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{results_dir}/{seed}/{start_state}/{trace}/{cluster}/{model}/{scheduler}/summary.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software2/splitwise-sim/notebooks/utils.py\u001b[0m(159)\u001b[0;36mget_summary_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    157 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    158 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 159 \u001b[0;31m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    160 \u001b[0;31m        \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{results_dir}/{seed}/{start_state}/{trace}/{cluster}/{model}/{scheduler}/summary.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    161 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software2/splitwise-sim/notebooks/utils.py\u001b[0m(160)\u001b[0;36mget_summary_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    158 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    159 \u001b[0;31m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 160 \u001b[0;31m        \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{results_dir}/{seed}/{start_state}/{trace}/{cluster}/{model}/{scheduler}/summary.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    161 \u001b[0;31m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    162 \u001b[0;31m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "--Call--\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(814)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    812 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    813 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 814 \u001b[0;31m@Appender(\n",
      "\u001b[0m\u001b[0;32m    815 \u001b[0;31m    _doc_read_csv_and_table.format(\n",
      "\u001b[0m\u001b[0;32m    816 \u001b[0;31m        \u001b[0mfunc_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"read_csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(884)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    882 \u001b[0;31m    \u001b[0mdtype_backend\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDtypeBackend\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoDefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    883 \u001b[0;31m) -> DataFrame | TextFileReader:\n",
      "\u001b[0m\u001b[0;32m--> 884 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0minfer_datetime_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    885 \u001b[0;31m        warnings.warn(\n",
      "\u001b[0m\u001b[0;32m    886 \u001b[0;31m            \u001b[0;34m\"The argument 'infer_datetime_format' is deprecated and will \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(895)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    893 \u001b[0;31m        )\n",
      "\u001b[0m\u001b[0;32m    894 \u001b[0;31m    \u001b[0;31m# locals() should never be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 895 \u001b[0;31m    \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    896 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filepath_or_buffer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    897 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(896)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    894 \u001b[0;31m    \u001b[0;31m# locals() should never be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    895 \u001b[0;31m    \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 896 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filepath_or_buffer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    897 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    898 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(897)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    895 \u001b[0;31m    \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    896 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"filepath_or_buffer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 897 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    898 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    899 \u001b[0;31m    kwds_defaults = _refine_defaults_read(\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(899)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    897 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    898 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 899 \u001b[0;31m    kwds_defaults = _refine_defaults_read(\n",
      "\u001b[0m\u001b[0;32m    900 \u001b[0;31m        \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    901 \u001b[0;31m        \u001b[0mdelimiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(900)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    898 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    899 \u001b[0;31m    kwds_defaults = _refine_defaults_read(\n",
      "\u001b[0m\u001b[0;32m--> 900 \u001b[0;31m        \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    901 \u001b[0;31m        \u001b[0mdelimiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    902 \u001b[0;31m        \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/disk1/futianhao/software1/miniconda3/envs/vllm02/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m(901)\u001b[0;36mread_csv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    899 \u001b[0;31m    kwds_defaults = _refine_defaults_read(\n",
      "\u001b[0m\u001b[0;32m    900 \u001b[0;31m        \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 901 \u001b[0;31m        \u001b[0mdelimiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    902 \u001b[0;31m        \u001b[0mdelim_whitespace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    903 \u001b[0;31m        \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/baseline/rr_conv_30/70_0/bloom-176b/token_jsq/summary.csv'\n",
      "Failed to read ../results/0/baseline/rr_conv_30/70_0/bloom-176b/token_jsq/summary.csv\n",
      "Failed to read ../results/0/baseline/rr_conv_30/70_0/bloom-176b/token_jsq/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/baseline/rr_conv_30/0_40/bloom-176b/token_jsq/summary.csv'\n",
      "Failed to read ../results/0/baseline/rr_conv_30/0_40/bloom-176b/token_jsq/summary.csv\n",
      "Failed to read ../results/0/baseline/rr_conv_30/0_40/bloom-176b/token_jsq/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_45_25/rr_conv_30/70_0/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_45_25/rr_conv_30/70_0/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_45_25/rr_conv_30/70_0/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_25_15/rr_conv_30/0_40/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_25_15/rr_conv_30/0_40/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_25_15/rr_conv_30/0_40/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_1_1/rr_conv_30/26_25/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_1_1/rr_conv_30/26_25/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_1_1/rr_conv_30/26_25/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwisehhcap_1_1/rr_conv_30/21_25/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwisehhcap_1_1/rr_conv_30/21_25/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwisehhcap_1_1/rr_conv_30/21_25/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/baseline/rr_conv_40/70_0/bloom-176b/token_jsq/summary.csv'\n",
      "Failed to read ../results/0/baseline/rr_conv_40/70_0/bloom-176b/token_jsq/summary.csv\n",
      "Failed to read ../results/0/baseline/rr_conv_40/70_0/bloom-176b/token_jsq/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/baseline/rr_conv_40/0_40/bloom-176b/token_jsq/summary.csv'\n",
      "Failed to read ../results/0/baseline/rr_conv_40/0_40/bloom-176b/token_jsq/summary.csv\n",
      "Failed to read ../results/0/baseline/rr_conv_40/0_40/bloom-176b/token_jsq/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_45_25/rr_conv_40/70_0/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_45_25/rr_conv_40/70_0/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_45_25/rr_conv_40/70_0/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_25_15/rr_conv_40/0_40/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_25_15/rr_conv_40/0_40/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_25_15/rr_conv_40/0_40/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_1_1/rr_conv_40/26_25/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_1_1/rr_conv_40/26_25/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_1_1/rr_conv_40/26_25/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwisehhcap_1_1/rr_conv_40/21_25/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwisehhcap_1_1/rr_conv_40/21_25/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwisehhcap_1_1/rr_conv_40/21_25/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/baseline/rr_conv_50/70_0/bloom-176b/token_jsq/summary.csv'\n",
      "Failed to read ../results/0/baseline/rr_conv_50/70_0/bloom-176b/token_jsq/summary.csv\n",
      "Failed to read ../results/0/baseline/rr_conv_50/70_0/bloom-176b/token_jsq/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/baseline/rr_conv_50/0_40/bloom-176b/token_jsq/summary.csv'\n",
      "Failed to read ../results/0/baseline/rr_conv_50/0_40/bloom-176b/token_jsq/summary.csv\n",
      "Failed to read ../results/0/baseline/rr_conv_50/0_40/bloom-176b/token_jsq/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_45_25/rr_conv_50/70_0/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_45_25/rr_conv_50/70_0/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_45_25/rr_conv_50/70_0/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_25_15/rr_conv_50/0_40/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_25_15/rr_conv_50/0_40/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_25_15/rr_conv_50/0_40/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "[Errno 2] No such file or directory: '../results/0/splitwise_1_1/rr_conv_50/26_25/bloom-176b/mixed_pool/summary.csv'\n",
      "Failed to read ../results/0/splitwise_1_1/rr_conv_50/26_25/bloom-176b/mixed_pool/summary.csv\n",
      "Failed to read ../results/0/splitwise_1_1/rr_conv_50/26_25/bloom-176b/mixed_pool/detailed/0.csv\n",
      "> \u001b[0;32m/tmp/ipykernel_33565/1625614644.py\u001b[0m(13)\u001b[0;36mget_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m            \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# fth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0msummary_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_summary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 汇总数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m            \u001b[0mrequest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_request_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 获取 请求数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     15 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0msummary_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 如果 汇总数据或请求数据为空，则跳过\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# isopower clusters, conv trace\n",
    "\n",
    "isopower_conv_configs = [\n",
    "    baseline_a100_config(70),\n",
    "    baseline_h100_config(40),\n",
    "    splitwise_aa_config(45, 25),\n",
    "    splitwise_hh_config(25, 15),\n",
    "    splitwise_ha_config(25, 26),\n",
    "    splitwise_hhcap_config(25, 21),\n",
    "]\n",
    "\n",
    "min_x = 50\n",
    "max_x = 181\n",
    "if model_to_plot == \"bloom-176b\":\n",
    "    min_x = 30\n",
    "    max_x = 161\n",
    "isopower_conv_traces = [f\"rr_conv_{i}_1min\" for i in range(80, 151, 10)]\n",
    "isopower_conv_traces = [f\"rr_conv_{i}\" for i in range(min_x, max_x, 10)]\n",
    "\n",
    "isopower_conv_results_df, isopower_conv_request_dfs = get_data(\n",
    "    isopower_conv_configs, isopower_conv_traces, seed=seed, model=model_to_plot)\n",
    "plot_y_vs_trace_new(isopower_conv_results_df,\n",
    "                    isopower_conv_traces,\n",
    "                    y_vars=[\"ttft_slowdown\", \"tbt_slowdown\", \"e2e_slowdown\"],\n",
    "                    #y_vars_labels=[\"TTFT\\nSlowdown\", \"TBT\\nSlowdown\", \"E2E\\nSlowdown\"],\n",
    "                    title=\"\")\n",
    "                    #title=\"Isopower Clusters, Conversation Trace\")\n",
    "axs = plt.gca()\n",
    "axs.set_xticks(ticks=range(0, len(isopower_conv_traces), 2), labels=[f\"{i}\" for i in range(min_x, max_x, 20)])\n",
    "#plt.subplots_adjust(left=0.)\n",
    "isopower_conv_results_df.to_csv(f\"{plotsdata_dir}/isopower_conv_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isopower_conv_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isopower clusters, code trace\n",
    "\n",
    "isopower_code_configs = [\n",
    "    baseline_a100_config(70),\n",
    "    baseline_h100_config(40),\n",
    "    splitwise_aa_config(55, 15),\n",
    "    splitwise_hh_config(35, 5),\n",
    "    splitwise_ha_config(35, 8),\n",
    "    splitwise_hhcap_config(35, 7),\n",
    "]\n",
    "\n",
    "min_x = 50\n",
    "max_x = 181\n",
    "if model_to_plot == \"bloom-176b\":\n",
    "    min_x = 30\n",
    "    max_x = 101\n",
    "isopower_code_traces = [f\"rr_code_{i}_1min\" for i in range(50, 121, 10)]\n",
    "isopower_code_traces = [f\"rr_code_{i}\" for i in range(min_x, max_x, 10)]\n",
    "\n",
    "isopower_code_results_df, isopower_code_request_dfs = get_data(\n",
    "    isopower_code_configs, isopower_code_traces, seed=seed, model=model_to_plot)\n",
    "plot_y_vs_trace_new(isopower_code_results_df,\n",
    "                    isopower_code_traces,\n",
    "                    y_vars=[\"ttft_slowdown\", \"tbt_slowdown\", \"e2e_slowdown\"],\n",
    "                    #y_vars_labels=[\"TTFT\\nSlowdown\", \"TBT\\nSlowdown\", \"E2E\\nSlowdown\"],\n",
    "                    title=\"\")\n",
    "                    #title=\"Isopower Clusters, Code Trace\")\n",
    "isopower_code_results_df.to_csv(f\"{plotsdata_dir}/isopower_code_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isopower_code_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code cluster for conv trace\n",
    "\n",
    "isopower_code_for_conv_results_df, isopower_code_for_conv_request_dfs = \\\n",
    "    get_data(isopower_code_configs, isopower_conv_traces, seed=seed, model=model_to_plot)\n",
    "plot_y_vs_trace_new_swapped(isopower_code_for_conv_results_df,\n",
    "                    isopower_conv_traces,\n",
    "                    y_vars=[\"ttft_slowdown\", \"tbt_slowdown\", \"e2e_slowdown\"],\n",
    "                    quantiles=[0.9],\n",
    "                    #y_vars_labels=[\"TTFT\\nSlowdown\", \"TBT\\nSlowdown\", \"E2E\\nSlowdown\"],\n",
    "                    title=\"\")\n",
    "axs = plt.gca()\n",
    "min_x = 50\n",
    "max_x = 181\n",
    "if model_to_plot == \"bloom-176b\":\n",
    "    min_x = 30 \n",
    "    max_x = 161\n",
    "axs.set_xticks(ticks=range(0, len(isopower_conv_traces), 2), labels=[f\"{i}\" for i in range(30, max_x, 20)])\n",
    "#plt.savefig(f\"{plots_dir}/isopower_code_for_conv_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "isopower_code_for_conv_results_df.to_csv(f\"{plotsdata_dir}/isopower_code_for_conv_{model_to_plot}_p90.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isopower_code_for_conv_{model_to_plot}_p90.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max_throughput(find_within_slo(isopower_code_for_conv_results_df, slos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max_throughput(find_within_slo(isopower_conv_results_df, slos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv cluster for code trace\n",
    "\n",
    "isopower_conv_for_code_results_df, isopower_conv_for_code_request_dfs = \\\n",
    "    get_data(isopower_conv_configs, isopower_code_traces, seed=seed, model=model_to_plot)\n",
    "plot_y_vs_trace_new(isopower_conv_for_code_results_df,\n",
    "                    isopower_code_traces,\n",
    "                    y_vars=[\"ttft_slowdown\", \"tbt_slowdown\", \"e2e_slowdown\"],\n",
    "                    y_vars_labels=[\"TTFT\\nSlowdown\", \"TBT\\nSlowdown\", \"E2E\\nSlowdown\"],\n",
    "                    title=\"Isopower Conversation Cluster, Code Trace\")\n",
    "max_x = 181\n",
    "if model_to_plot == \"bloom-176b\":\n",
    "    max_x = 141\n",
    "isopower_conv_for_code_results_df.to_csv(f\"{plotsdata_dir}/isopower_conv_for_code_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isopower_conv_for_code_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "#axs.set_xticks(ticks=range(0, len(isopower_code_traces), 2), labels=[f\"{i}\" for i in range(50, max_x, 20)])\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max_throughput(find_within_slo(isopower_code_results_df, slos))[[\"name\", \"trace\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max_throughput(find_within_slo(isopower_conv_for_code_results_df, slos))[[\"name\", \"trace\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isocost clusters, conv trace\n",
    "\n",
    "isocost_conv_configs = [\n",
    "    baseline_a100_config(86),\n",
    "    baseline_h100_config(40),\n",
    "    splitwise_aa_config(51, 35),\n",
    "    splitwise_hh_config(25, 15),\n",
    "    splitwise_ha_config(30, 21),\n",
    "    splitwise_hhcap_config(30, 10),\n",
    "]\n",
    "\n",
    "max_x = 181\n",
    "if model_to_plot == \"bloom-176b\":\n",
    "    max_x = 171\n",
    "isocost_conv_traces = [f\"rr_conv_{i}_1min\" for i in range(80, 151, 10)]\n",
    "isocost_conv_traces = [f\"rr_conv_{i}\" for i in range(50, max_x, 10)]\n",
    "\n",
    "isocost_conv_results_df, isocost_conv_request_dfs = get_data(\n",
    "    isocost_conv_configs, isocost_conv_traces, seed=seed, model=model_to_plot)\n",
    "plt.close()\n",
    "plot_y_vs_trace_new(isocost_conv_results_df,\n",
    "                    isocost_conv_traces,\n",
    "                    y_vars=[\"ttft_slowdown\", \"tbt_slowdown\", \"e2e_slowdown\"],\n",
    "                    y_vars_labels=[\"TTFT\\nSlowdown\", \"TBT\\nSlowdown\", \"E2E\\nSlowdown\"],\n",
    "                    title=\"Isocost Clusters, Conversation Trace\")\n",
    "axs = plt.gca()\n",
    "axs.set_xticks(ticks=range(0, len(isocost_conv_traces), 2), labels=[f\"{i}\" for i in range(50, max_x, 20)])\n",
    "isocost_conv_results_df.to_csv(f\"{plotsdata_dir}/isocost_conv_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isocost_conv_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isocost clusters, code trace\n",
    "\n",
    "isocost_code_configs = [\n",
    "    baseline_a100_config(86),\n",
    "    baseline_h100_config(40),\n",
    "    splitwise_aa_config(66, 20),\n",
    "    splitwise_hh_config(35, 5),\n",
    "    splitwise_ha_config(35, 10),\n",
    "    splitwise_hhcap_config(35, 7),\n",
    "]\n",
    "\n",
    "max_x = 181\n",
    "if model_to_plot == \"bloom-176b\":\n",
    "    max_x = 121\n",
    "isocost_code_traces = [f\"rr_code_{i}_1min\" for i in range(50, 121, 10)]\n",
    "isocost_code_traces = [f\"rr_code_{i}\" for i in range(50, max_x, 10)]\n",
    "\n",
    "isocost_code_results_df, isocost_code_request_dfs = get_data(\n",
    "    isocost_code_configs, isocost_code_traces, seed=seed, model=model_to_plot)\n",
    "plot_y_vs_trace_new(isocost_code_results_df,\n",
    "                    isocost_code_traces,\n",
    "                    y_vars=[\"ttft_slowdown\", \"tbt_slowdown\", \"e2e_slowdown\"],\n",
    "                    y_vars_labels=[\"TTFT\\nSlowdown\", \"TBT\\nSlowdown\", \"E2E\\nSlowdown\"],\n",
    "                    title=\"Isocost Clusters, Code Trace\")\n",
    "isocost_code_results_df.to_csv(f\"{plotsdata_dir}/isocost_code_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isocost_code_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost and max.throughput for isopower configurations\n",
    "\n",
    "plot_cost_throughput_optimal_within_slo(isopower_conv_results_df, \"Baseline-A100\", slos)\n",
    "isopower_conv_results_df.to_csv(f\"{plotsdata_dir}/isopower_conv_cost_throughput_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isopower_conv_cost_throughput_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plot_cost_throughput_optimal_within_slo(isopower_code_results_df, \"Baseline-A100\", slos)\n",
    "isopower_code_results_df.to_csv(f\"{plotsdata_dir}/isopower_code_cost_throughput_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isopower_code_cost_throughput_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power and max. throughput for isopower configurations\n",
    "\n",
    "plot_power_throughput_optimal_within_slo(isocost_conv_results_df, \"Baseline-A100\", slos)\n",
    "isocost_conv_results_df.to_csv(f\"{plotsdata_dir}/isocost_conv_power_throughput_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isocost_conv_power_throughput_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plot_power_throughput_optimal_within_slo(isocost_code_results_df, \"Baseline-A100\", slos)\n",
    "isocost_code_results_df.to_csv(f\"{plotsdata_dir}/isocost_code_power_throughput_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isocost_code_power_throughput_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provisioning sweep\n",
    "\n",
    "configs = []\n",
    "\n",
    "min_a100 = 1\n",
    "max_a100 = 70\n",
    "min_h100 = 1\n",
    "max_h100 = 40\n",
    "\n",
    "for num_a100 in range(min_a100, 2*max_a100, 1):\n",
    "    configs.append(baseline_a100_config(num_a100))\n",
    "\n",
    "for num_h100 in range(min_h100, 2*max_h100, 1):\n",
    "    configs.append(baseline_h100_config(num_h100))\n",
    "\n",
    "for num_prompts in range(min_a100, max_a100, 1):\n",
    "    for num_tokens in range(min_a100, max_a100, 1):\n",
    "        configs.append(splitwise_aa_config(num_prompts, num_tokens))\n",
    "\n",
    "for num_prompts in range(min_h100, max_h100, 1):\n",
    "    for num_tokens in range(min_h100, max_h100, 1):\n",
    "        configs.append(splitwise_hh_config(num_prompts, num_tokens))\n",
    "\n",
    "for num_prompts in range(min_h100, max_h100, 1):\n",
    "    for num_tokens in range(min_a100, max_a100, 1):\n",
    "        configs.append(splitwise_ha_config(num_prompts, num_tokens))\n",
    "\n",
    "for num_prompts in range(min_h100, max_h100, 1):\n",
    "    for num_tokens in range(min_h100, max_h100, 1):\n",
    "        configs.append(splitwise_hhcap_config(num_prompts, num_tokens))\n",
    "\n",
    "provision_code_trace = [\"rr_code_70_2min\"]\n",
    "#provision_conv_trace = [\"rr_conv_70_2min\"]\n",
    "\n",
    "provision_code_df, provision_code_request_dfs = get_data(configs, traces=provision_code_trace, seed=seed, model=model_to_plot)\n",
    "#provision_conv_df, provision_conv_request_dfs = get_data(configs, traces=provision_conv_trace, seed=seed, model=model_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_provision_sweep(result_df):\n",
    "    subset_df = result_df\n",
    "\n",
    "    # plot scatter of num_prompts vs num_tokens for the ttft times\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(9, 7), constrained_layout=True)\n",
    "\n",
    "    title_dict = {\n",
    "        \"ttft_slowdown\": \"TTFT Slowdown\",\n",
    "        \"tbt_slowdown\": \"TBT Slowdown\",\n",
    "        \"e2e_slowdown\": \"E2E Slowdown\"\n",
    "    }\n",
    "\n",
    "    y_vars = [\"ttft_slowdown\", \"tbt_slowdown\", \"e2e_slowdown\"]\n",
    "    quantiles = [0.5, 0.9, 0.99]\n",
    "\n",
    "    cmap = plt.get_cmap('viridis_r')\n",
    "    colors = cmap(np.arange(cmap.N))\n",
    "\n",
    "    max_slowdown = max(subset_df[\"ttft_slowdown_p99\"].max(), subset_df[\"tbt_slowdown_p99\"].max(), subset_df[\"e2e_slowdown_p99\"].max())\n",
    "    max_slo = 6\n",
    "    colors[-1] = [0, 0, 0, 1]  # RGBA for black\n",
    "    new_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "    all_slo_df = subset_df\n",
    "    for y_var in y_vars:\n",
    "        for quantile in quantiles:\n",
    "            all_slo_df = all_slo_df[all_slo_df[f\"{y_var}_p{int(quantile * 100)}\"] <= get_slo(y_var, quantile)]\n",
    "\n",
    "    for y_var in y_vars:\n",
    "        for quantile in quantiles:\n",
    "            sc = axs[y_vars.index(y_var)][quantiles.index(quantile)].scatter(\n",
    "                subset_df[\"num_prompts\"],\n",
    "                subset_df[\"num_tokens\"],\n",
    "                c=subset_df[f\"{y_var}_p{int(quantile * 100)}\"],\n",
    "                #norm=mcolors.LogNorm(vmin=1, vmax=max_slo),\n",
    "                norm=mcolors.Normalize(vmin=1, vmax=max_slo),\n",
    "                cmap=new_cmap\n",
    "                #cmap='viridis_r'\n",
    "            )\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].set_xlabel(\"# Prompt Machines\")\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].set_ylabel(\"# Token Machines\")\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].set_title(f\"p{int(quantile * 100)} {title_dict[y_var]}\")\n",
    "            #axs[y_vars.index(y_var)][quantiles.index(quantile)].legend(bbox_to_anchor=(0.5, 1.11), loc=\"lower center\", ncol=3, fontsize=7)\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].grid()\n",
    "            \n",
    "            slo = get_slo(y_var, quantile)\n",
    "            # Filter the DataFrame to only include rows that meet the SLO\n",
    "            slo_df = subset_df[subset_df[f\"{y_var}_p{int(quantile * 100)}\"] <= slo]\n",
    "\n",
    "            # Group by 'num_prompts' and find the minimum 'num_tokens' for each group\n",
    "            frontier_df = slo_df.groupby('num_prompts')['num_tokens'].min().reset_index()\n",
    "\n",
    "            # Plot the frontier points\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].scatter(\n",
    "                frontier_df[\"num_prompts\"],\n",
    "                frontier_df[\"num_tokens\"],\n",
    "                c='red',  # Color of the frontier points\n",
    "                label='SLO Frontier'\n",
    "            )\n",
    "\n",
    "            # Find the cost optimal and power optimal points that meet the SLO\n",
    "            #cost_optimal_point = slo_df.loc[slo_df['cost'].idxmin()]\n",
    "            #power_optimal_point = slo_df.loc[slo_df['power'].idxmin()]\n",
    "            cost_optimal_point = all_slo_df.loc[all_slo_df['cost'].idxmin()]\n",
    "            power_optimal_point = all_slo_df.loc[all_slo_df['power'].idxmin()]\n",
    "\n",
    "            # Plot the cost optimal point\n",
    "            axs[y_vars.index(y_var)][quantiles.index(quantile)].scatter(\n",
    "                cost_optimal_point[\"num_prompts\"],\n",
    "                cost_optimal_point[\"num_tokens\"],\n",
    "                c='azure',  # Color of the cost optimal point\n",
    "                s=120,\n",
    "                label='Cost Optimal',\n",
    "                marker='*',\n",
    "                edgecolors='black',\n",
    "            )\n",
    "\n",
    "            # Plot the power optimal point\n",
    "            #axs[y_vars.index(y_var)][quantiles.index(quantile)].scatter(\n",
    "            #    power_optimal_point[\"num_prompts\"],\n",
    "            #    power_optimal_point[\"num_tokens\"],\n",
    "            #    c='green',  # Color of the power optimal point\n",
    "            #    label='Power Optimal'\n",
    "            #)\n",
    "\n",
    "            #axs[y_vars.index(y_var)][quantiles.index(quantile)].legend()\n",
    "\n",
    "            # add colorbar\n",
    "            #plt.colorbar(sc, ax=axs[y_vars.index(y_var)][quantiles.index(quantile)])\n",
    "            # add colorbar with custom tick labels\n",
    "            cbar = plt.colorbar(sc, ax=axs[y_vars.index(y_var)][quantiles.index(quantile)])\n",
    "            tick_locs = list(cbar.get_ticks())\n",
    "            #tick_locs.append(slo)\n",
    "            cbar.set_ticks(tick_locs)\n",
    "            tick_labels = [str(int(tick)) if tick < max_slo else f'{max_slo}+' for tick in tick_locs]\n",
    "            #tick_labels[-1] = 'SLO'\n",
    "            cbar.set_ticklabels(tick_labels)\n",
    "            cbar.ax.axhline(get_slo(y_var, quantile), color='red', linewidth=3)\n",
    "\n",
    "            # Add SLO label to the left of the colorbar\n",
    "            cbar.set_label('SLO', rotation=0, labelpad=0, color='red')\n",
    "            slo_position = (slo - 1) / (max_slo - 1)\n",
    "            cbar.ax.yaxis.set_label_coords(-1.7, slo_position + 0.05)\n",
    "    print(cost_optimal_point[\"num_prompts\"], cost_optimal_point[\"num_tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_provision_sweep(provision_code_df[provision_code_df[\"system\"] == \"Splitwise-HH\"])\n",
    "plt.savefig(f\"{plots_dir}/hh_provision_code_sweep_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provision_code_df.to_csv(f\"{plotsdata_dir}/provision_code_sweep_{model_to_plot}.csv\", index=False)\n",
    "provision_conv_df.to_csv(f\"{plotsdata_dir}/provision_conv_sweep_{model_to_plot}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isothroughput_code_cost_df = plot_isothroughput_cost_optimal_within_slo(provision_code_df, \"Baseline-A100\", slos)\n",
    "isothroughput_code_cost_df.to_csv(f\"{plotsdata_dir}/isothroughput_code_cost_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isothroughput_code_cost_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "isothroughput_code_power_df = plot_isothroughput_power_optimal_within_slo(provision_code_df, \"Baseline-A100\", slos)\n",
    "isothroughput_code_power_df.to_csv(f\"{plotsdata_dir}/isothroughput_code_power_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isothroughput_code_power_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "isothroughout_code_count_df = plot_isothroughput_count_optimal_within_slo(provision_code_df, \"Baseline-A100\", slos)\n",
    "isothroughout_code_count_df.to_csv(f\"{plotsdata_dir}/isothroughput_code_count_{model_to_plot}.csv\", index=False)\n",
    "plt.savefig(f\"{plots_dir}/isothroughput_code_count_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isothroughput_conv_cost_df = plot_isothroughput_cost_optimal_within_slo(provision_conv_df, \"Baseline-A100\", slos)\n",
    "isothroughput_conv_cost_df.to_csv(f\"{plotsdata_dir}/isothroughput_conv_cost_{model_to_plot}.csv\")\n",
    "plt.savefig(f\"{plots_dir}/isothroughput_conv_cost_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "isothroughput_conv_power_df = plot_isothroughput_power_optimal_within_slo(provision_conv_df, \"Baseline-A100\", slos)\n",
    "isothroughput_conv_power_df.to_csv(f\"{plotsdata_dir}/isothroughput_conv_power_{model_to_plot}.csv\")\n",
    "plt.savefig(f\"{plots_dir}/isothroughput_conv_power_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "isothroughput_conv_count_df = plot_isothroughput_count_optimal_within_slo(provision_conv_df, \"Baseline-A100\", slos)\n",
    "isothroughput_conv_count_df.to_csv(f\"{plotsdata_dir}/isothroughput_conv_count_{model_to_plot}.csv\")\n",
    "plt.savefig(f\"{plots_dir}/isothroughput_conv_count_{model_to_plot}.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_least_count(find_within_slo(provision_conv_df, slos)).drop_duplicates(subset=[\"system\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_least_power(find_within_slo(provision_code_df, slos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_throughput(request_nodes_df, t_start=60, t_end=540):\n",
    "    \"\"\"\n",
    "    Finds the throughput of a request_df between t_start and t_end\n",
    "    \"\"\"\n",
    "    return request_nodes_df[\n",
    "            (request_nodes_df[\"node_type\"] == \"TOKEN\") & \\\n",
    "            (request_nodes_df[\"completion_timestamp\"] >= t_start) & \\\n",
    "            (request_nodes_df[\"completion_timestamp\"] < t_end)][\"request_id\"].nunique() / (t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in isocost_conv_configs:\n",
    "    request_nodes_df = get_request_nodes_with_config(results_dir, config, \"rr_conv_180\", seed=0, model=model_to_plot)\n",
    "    throughput = find_throughput(request_nodes_df)\n",
    "    print(f\"{config['name']}: {throughput} rps, ${round(config['cost'], 2)}, {round(throughput/config['cost'], 2)} rps/$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_within_slo = find_within_slo(provision_code_df, slos)\n",
    "cheapest_configs = find_cheapest(configs_within_slo)\n",
    "least_power_configs = find_least_power(configs_within_slo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_num_batch_tokens(config, trace, seed, model=\"\"):\n",
    "    df = get_instances_data_with_config(results_dir, config, trace, seed, model)\n",
    "    if \"splitwise\" in config.get(\"start_state\", \"\"):\n",
    "        num_prompt_batch_tokens, num_token_batch_tokens = get_num_batch_tokens_splitwise(df)\n",
    "        sns.ecdfplot(num_prompt_batch_tokens, label=\"Prompt\")\n",
    "        sns.ecdfplot(num_token_batch_tokens, label=\"Token\")\n",
    "    else:\n",
    "        num_batch_tokens = get_num_batch_tokens(df)\n",
    "        sns.ecdfplot(num_batch_tokens, label=\"Batch\")\n",
    "    plt.xlabel(\"Number of Batch Tokens\")\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.legend()\n",
    "    plt.title(f'{config[\"name\"]} {config[\"scheduler\"]}, {trace}')\n",
    "    plt.grid()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duration_ccdf_batch_tokens(config, trace, seed, model=\"\"):\n",
    "    df = get_instances_data_with_config(results_dir, config, trace, seed, model)\n",
    "    grouped_df = df[[\"batch_tokens\", \"duration\"]].groupby(\"batch_tokens\").sum()\n",
    "\n",
    "    # find idle durations between iteration_end and iteration_start of next row in df for each instance identified by name\n",
    "    total_idle_duration = 0\n",
    "    for name in df[\"name\"].unique():\n",
    "        inst_df = df[df[\"name\"] == name].sort_values(\"iteration_start\")\n",
    "        inst_df[\"idle_duration\"] = inst_df[\"iteration_start\"] - inst_df[\"iteration_end\"].shift(1)\n",
    "        inst_df[\"idle_duration\"] = inst_df[\"idle_duration\"].fillna(0)\n",
    "        total_idle_duration += inst_df[\"idle_duration\"].sum()\n",
    "\n",
    "    # add entry to grouped_df at index 0\n",
    "    grouped_df.loc[0] = [total_idle_duration]\n",
    "\n",
    "    # sortby grouped_df.index\n",
    "    grouped_df = grouped_df.sort_index()\n",
    "    grouped_df[\"cum_duration\"] = np.cumsum(grouped_df[\"duration\"])\n",
    "    grouped_df[\"cum_duration\"] /= grouped_df[\"cum_duration\"].max()\n",
    "    # plot cumulative cdf of duration vs number of batch tokens\n",
    "    #plt.plot(grouped_df.index, np.cumsum(grouped_df[\"duration\"]))\n",
    "    plt.plot(grouped_df.index, grouped_df[\"cum_duration\"])\n",
    "    plt.xlabel(\"Number of Batch Tokens\")\n",
    "    plt.ylabel(\"Cumulative Duration (s)\")\n",
    "    plt.grid()\n",
    "    plt.title(f'{config[\"name\"]} {config[\"scheduler\"]}, {trace}')\n",
    "\n",
    "    #if \"splitwise\" in config.get(\"start_state\", \"\"):\n",
    "    #    n_prompt = int(config[\"start_state\"].split(\"_\")[1])\n",
    "    #    num_prompt_batch_tokens, num_token_batch_tokens = get_num_batch_tokens_splitwise(df, n_prompt)\n",
    "    #    sns.ecdfplot(num_prompt_batch_tokens, label=\"Prompt\")\n",
    "    #    sns.ecdfplot(num_token_batch_tokens, label=\"Token\")\n",
    "    #else:\n",
    "    #    num_batch_tokens = get_num_batch_tokens(df)\n",
    "    #    sns.ecdfplot(num_batch_tokens, label=\"Batch\")\n",
    "    #plt.xlabel(\"Number of Batch Tokens\")\n",
    "    #plt.ylabel(\"CDF\")\n",
    "    #plt.legend()\n",
    "    #plt.grid()\n",
    "    #return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idle_duration(df):\n",
    "    total_idle_duration = 0\n",
    "    for name in df[\"name\"].unique():\n",
    "        inst_df = df[df[\"name\"] == name].sort_values(\"iteration_start\")\n",
    "        inst_df[\"idle_duration\"] = inst_df[\"iteration_start\"] - inst_df[\"iteration_end\"].shift(1)\n",
    "        inst_df[\"idle_duration\"] = inst_df[\"idle_duration\"].fillna(0)\n",
    "        total_idle_duration += inst_df[\"idle_duration\"].sum()\n",
    "    return total_idle_duration\n",
    "\n",
    "def compare_duration_ccdf_batch_tokens(baseline_config, splitwise_config, trace, seed, model=\"\"):\n",
    "    baseline_df = get_instances_data_with_config(results_dir, baseline_config, trace, seed, model)\n",
    "    splitwise_df = get_instances_data_with_config(results_dir, splitwise_config, trace, seed, model)\n",
    "\n",
    "    # for baseline\n",
    "    # find idle durations between iteration_end and iteration_start of next row in df for each instance identified by name\n",
    "    baseline_idle_duration = find_idle_duration(baseline_df)\n",
    "    # add entry to grouped df at index 0\n",
    "    grouped_baseline_df = baseline_df[[\"batch_tokens\", \"duration\"]].groupby(\"batch_tokens\").sum()\n",
    "    grouped_baseline_df.loc[0] = [baseline_idle_duration]\n",
    "\n",
    "    # for splitwise, separate into prompt and token servers, and repeat the same process\n",
    "    # find idle durations between iteration_end and iteration_start of next row in df for each instance identified by name\n",
    "    splitwise_prompt_df = splitwise_df[splitwise_df[\"tag\"] == \"prompt\"]\n",
    "    splitwise_token_df = splitwise_df[splitwise_df[\"tag\"] == \"token\"]\n",
    "    splitwise_prompt_idle_duration = find_idle_duration(splitwise_prompt_df)\n",
    "    splitwise_token_idle_duration = find_idle_duration(splitwise_token_df)\n",
    "    # add entry to grouped df at index 0\n",
    "    grouped_splitwise_prompt_df = splitwise_prompt_df[[\"batch_tokens\", \"duration\"]].groupby(\"batch_tokens\").sum()\n",
    "    grouped_splitwise_prompt_df.loc[0] = [splitwise_prompt_idle_duration]\n",
    "    grouped_splitwise_token_df = splitwise_token_df[[\"batch_tokens\", \"duration\"]].groupby(\"batch_tokens\").sum()\n",
    "    grouped_splitwise_token_df.loc[0] = [splitwise_token_idle_duration]\n",
    "\n",
    "    # sortby grouped_df.index\n",
    "    grouped_baseline_df = grouped_baseline_df.sort_index()\n",
    "    grouped_baseline_df[\"cum_duration\"] = np.cumsum(grouped_baseline_df[\"duration\"])\n",
    "    grouped_baseline_df[\"cum_duration\"] /= grouped_baseline_df[\"cum_duration\"].max()\n",
    "\n",
    "    grouped_splitwise_prompt_df = grouped_splitwise_prompt_df.sort_index()\n",
    "    grouped_splitwise_prompt_df[\"cum_duration\"] = np.cumsum(grouped_splitwise_prompt_df[\"duration\"])\n",
    "    grouped_splitwise_prompt_df[\"cum_duration\"] /= grouped_splitwise_prompt_df[\"cum_duration\"].max()\n",
    "    \n",
    "    grouped_splitwise_token_df = grouped_splitwise_token_df.sort_index()\n",
    "    grouped_splitwise_token_df[\"cum_duration\"] = np.cumsum(grouped_splitwise_token_df[\"duration\"])\n",
    "    grouped_splitwise_token_df[\"cum_duration\"] /= grouped_splitwise_token_df[\"cum_duration\"].max()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(3, 3), constrained_layout=True)\n",
    "\n",
    "    # plot cumulative cdf of duration vs number of batch tokens\n",
    "    #plt.plot(grouped_df.index, np.cumsum(grouped_df[\"duration\"]))\n",
    "    axs.plot(grouped_baseline_df.index, grouped_baseline_df[\"cum_duration\"], label=\"Baseline-H100\")\n",
    "    axs.plot(grouped_splitwise_prompt_df.index, grouped_splitwise_prompt_df[\"cum_duration\"], label=\"Splitwise-HH (Prompt)\")\n",
    "    axs.plot(grouped_splitwise_token_df.index, grouped_splitwise_token_df[\"cum_duration\"], label=\"Splitwise-HH (Token)\")\n",
    "\n",
    "    axs.set_xlabel(\"Number of Batch Tokens\")\n",
    "    axs.set_ylabel(\"Cumulative Duration\")\n",
    "    axs.grid()\n",
    "    axs.set_xscale(\"log\")\n",
    "    axs.legend(loc=\"lower center\", ncol=1, bbox_to_anchor=(0.5, 1.0))\n",
    "    #ax.title(f'{config[\"name\"]} {config[\"scheduler\"]}, {trace}')\n",
    "    \n",
    "    # return dataframe\n",
    "    return baseline_df, splitwise_df\n",
    "\n",
    "    #if \"splitwise\" in config.get(\"start_state\", \"\"):\n",
    "    #    n_prompt = int(config[\"start_state\"].split(\"_\")[1])\n",
    "    #    num_prompt_batch_tokens, num_token_batch_tokens = get_num_batch_tokens_splitwise(df, n_prompt)\n",
    "    #    sns.ecdfplot(num_prompt_batch_tokens, label=\"Prompt\")\n",
    "    #    sns.ecdfplot(num_token_batch_tokens, label=\"Token\")\n",
    "    #else:\n",
    "    #    num_batch_tokens = get_num_batch_tokens(df)\n",
    "    #    sns.ecdfplot(num_batch_tokens, label=\"Batch\")\n",
    "    #plt.xlabel(\"Number of Batch Tokens\")\n",
    "    #plt.ylabel(\"CDF\")\n",
    "    #plt.legend()\n",
    "    #plt.grid()\n",
    "    #return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df, splitwise_df = compare_duration_ccdf_batch_tokens(\n",
    "    baseline_h100_config(40),\n",
    "    splitwise_hh_config(25, 15),\n",
    "    #\"rr_conv_70\",\n",
    "    \"rr_conv_130\",\n",
    "    seed=0,\n",
    "    model=model_to_plot)\n",
    "baseline_df.to_csv(os.path.join(plotsdata_dir, f\"duration_ccdf_batch_tokens_baseline_{model_to_plot}_conv130.csv\"))\n",
    "splitwise_df.to_csv(os.path.join(plotsdata_dir, f\"duration_ccdf_batch_tokens_splitwise_{model_to_plot}_conv130.csv\"))\n",
    "#plt.savefig(os.path.join(plots_dir, f\"duration_ccdf_batch_tokens_{model_to_plot}_conv70.pdf\"))\n",
    "plt.savefig(os.path.join(plots_dir, f\"duration_ccdf_batch_tokens_{model_to_plot}_conv130.pdf\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sweep(results_df, title=None, x=\"num_prompts\"):\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(6, 4), sharex=True, constrained_layout=True)\n",
    "    sns.lineplot(data=results_df, x=x, y=\"ttft_slowdown_p90\", style=\"system\", markers=True, markersize=7, ax=axs[0][0])\n",
    "    sns.lineplot(data=results_df, x=x, y=\"tbt_slowdown_p90\", style=\"system\", markers=True, markersize=7, ax=axs[1][0])\n",
    "    sns.lineplot(data=results_df, x=x, y=\"e2e_slowdown_p90\", style=\"system\", markers=True, markersize=7, ax=axs[2][0])\n",
    "    sns.lineplot(data=results_df, x=x, y=\"ttft_slowdown_p50\", style=\"system\", markers=True, markersize=7, ax=axs[0][1])\n",
    "    sns.lineplot(data=results_df, x=x, y=\"tbt_slowdown_p50\", style=\"system\", markers=True, markersize=7, ax=axs[1][1])\n",
    "    sns.lineplot(data=results_df, x=x, y=\"e2e_slowdown_p50\", style=\"system\", markers=True, markersize=7, ax=axs[2][1])\n",
    "\n",
    "    for ax in axs.flatten():\n",
    "        ax.grid()\n",
    "        ax.set_xlabel(f\"{x} servers\")\n",
    "\n",
    "    #sns.move_legend(axs[0][0], \"lower center\", bbox_to_anchor=(0.5, 1.05), ncol=2, title=\"\")\n",
    "    axs[0][0].get_legend().set_visible(False)\n",
    "    axs[1][0].get_legend().set_visible(False)\n",
    "    axs[2][0].get_legend().set_visible(False)\n",
    "    axs[0][1].get_legend().set_visible(False)\n",
    "    axs[1][1].get_legend().set_visible(False)\n",
    "    axs[2][1].get_legend().set_visible(False)\n",
    "\n",
    "    # create a single legend in center of figure\n",
    "    handles, labels = axs[0][0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=3, title=\"\", bbox_to_anchor=(0.5, 1.01))\n",
    "\n",
    "    #axs[0][0].set_yscale(\"log\")\n",
    "    #axs[0][1].set_yscale(\"log\")\n",
    "    #axs[2][0].set_yscale(\"log\")\n",
    "    #axs[2][1].set_yscale(\"log\")\n",
    "    #axs[0][0].set_ylim(bottom=0, top=2)\n",
    "    #axs[0][1].set_ylim(bottom=0, top=2)\n",
    "\n",
    "    # set same y axis limits for plots in the same row (same as 0th column)\n",
    "    #axs[0][1].set_ylim(axs[0][0].get_ylim())\n",
    "    #axs[1][1].set_ylim(axs[1][0].get_ylim())\n",
    "    #axs[2][1].set_ylim(axs[2][0].get_ylim())\n",
    "\n",
    "\n",
    "    axs[0][0].set_ylabel(\"p90 TTFT (s)\")\n",
    "    axs[1][0].set_ylabel(\"p90 TBT (s)\")\n",
    "    #axs[2][0].set_ylabel(\"p99 Nth Token (s)\")\n",
    "    axs[2][0].set_ylabel(\"p90 e2e (s)\")\n",
    "    axs[0][1].set_ylabel(\"p50 TTFT (s)\")\n",
    "    axs[1][1].set_ylabel(\"p50 TBT (s)\")\n",
    "    axs[2][1].set_ylabel(\"p50 e2e (s)\")\n",
    "    #axs[2][1].set_ylabel(\"p50 Nth Token (s)\")\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=12)\n",
    "\n",
    "    # set 300dpi\n",
    "    plt.gcf().set_dpi(300)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isopower sweeps\n",
    "\n",
    "configs = []\n",
    "\n",
    "max_h100 = 40\n",
    "for num_h100 in range(5, max_h100, 5):\n",
    "    #num_a100 = int(10200 * (max_h100 - num_h100) // 6500)\n",
    "    num_a100 = int(44 * (max_h100 - num_h100) // 24.8)\n",
    "    configs.append(splitwise_ha_config(num_h100, num_a100))\n",
    "\n",
    "for num_h100 in range(5, max_h100, 5):\n",
    "    num_prompts = num_h100\n",
    "    num_tokens = max_h100 - num_prompts\n",
    "    configs.append(splitwise_hh_config(num_prompts, num_tokens))\n",
    "    configs.append(splitwise_hhcap_config(num_prompts, num_tokens))\n",
    "\n",
    "max_a100 = 70\n",
    "for num_prompts in range(5, max_a100, 5):\n",
    "    num_tokens = max_a100 - num_prompts\n",
    "    configs.append(splitwise_aa_config(num_prompts, num_tokens))\n",
    "\n",
    "#traces = [\"rr_conv_70\"]\n",
    "traces = [\"rr_code_100\"]\n",
    "\n",
    "results_df, request_dfs = get_data(configs, traces, seed=seed, model=model_to_plot)\n",
    "plot_sweep(results_df, title=f\"Isopower sweeps, {traces[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isocost sweeps\n",
    "\n",
    "configs = []\n",
    "\n",
    "max_h100 = 40\n",
    "for num_h100 in range(5, max_h100, 5):\n",
    "    #num_a100 = int(10200 * (max_h100 - num_h100) // 6500)\n",
    "    num_a100 = int(4.76 * (max_h100 - num_h100) // 2.21)\n",
    "    configs.append(splitwise_ha_config(num_h100, num_a100))\n",
    "\n",
    "for num_h100 in range(5, max_h100, 5):\n",
    "    num_prompts = num_h100\n",
    "    num_tokens = max_h100 - num_prompts\n",
    "    configs.append(splitwise_hh_config(num_prompts, num_tokens))\n",
    "\n",
    "max_a100 = 86\n",
    "for num_tokens in range(5, max_a100, 5):\n",
    "    num_prompts = max_a100 - num_tokens\n",
    "    configs.append(splitwise_aa_config(num_prompts, num_tokens))\n",
    "\n",
    "#traces = [\"rr_conv_70\"]\n",
    "traces = [\"rr_code_90\"]\n",
    "\n",
    "results_df, request_dfs = get_data(configs, traces, seed=seed, model=model_to_plot)\n",
    "plot_sweep(results_df, title=f\"Isocost sweeps, {traces[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
